{{- $teams := include "opentelemetry-targetallocator-generator.teams" . | fromYaml }}
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: {{ include "opentelemetry-targetallocator-generator.targetAllocatorName" . }}
  namespace: {{ .Release.Namespace }}
spec:
  mode: statefulset
  replicas: {{ .Values.replicas | int }}
  serviceAccount: {{ include "opentelemetry-targetallocator-generator.targetAllocatorName" . }}
  env:
  {{- range $teamName, $teamInfo := $teams }}
  - name: NR_LICENSE_KEY_{{ $teamName | upper | replace "-" "_" }}
    valueFrom:
      secretKeyRef:
        name: {{ $teamInfo.licenseKey.secretRef.name }}
        key: {{ $teamInfo.licenseKey.secretRef.key }}
  {{- end }}

  # Annotations for the pods
  podAnnotations:
    prometheus.io/scrape: "false" # This should be false by default. Otherwise 'otelcollector' self scraper job AND 'kubernetes-pods' scrape job will both scrape
  {{- range $key, $val := .Values.annotations }}
    {{ $key }}: {{ $val }}
  {{- end }}

  targetAllocator:
    enabled: true
    serviceAccount: {{ include "opentelemetry-targetallocator-generator.targetAllocatorName" . }}
    prometheusCR:
      enabled: {{ .Values.targetAllocator.prometheusCR.enabled | default false -}}
      {{ if .Values.targetAllocator.podMonitorSelector }}
      podMonitorSelector: {{ toYaml .Values.targetAllocator.podMonitorSelector | nindent 8 -}}
      {{ end }}
      {{- if .Values.targetAllocator.serviceMonitorSelector }}
      serviceMonitorSelector: {{ toYaml .Values.targetAllocator.serviceMonitorSelector | nindent 8 -}}
      {{ end }}

  # Security context for container priviliges
  {{- with .Values.securityContext }}
  securityContext: {{ toYaml . | nindent 4 }}
  {{- end }}

  # Ports to expose per service
  ports:
    - name: prometheus
      protocol: TCP
      port: {{ .Values.ports.prometheus.port }}
      targetPort: {{ .Values.ports.prometheus.targetPort }}

  # Image
  image: "{{ .Values.image.repository }}:{{ include "opentelemetry-targetallocator-generator.targetAllocatorImageTag" . }}"
  imagePullPolicy: {{ .Values.image.pullPolicy }}

  # Resources
  resources:
    requests:
      cpu: {{ .Values.resources.requests.cpu }}
      memory: {{ .Values.resources.requests.memory }}
    limits:
      cpu: {{ .Values.resources.limits.cpu }}
      memory: {{ .Values.resources.limits.memory }}

  config:
    receivers:
      prometheus:
        config:
          scrape_configs:
          - job_name: 'otel-collector'
            scrape_interval: 30s
            static_configs:
            - targets: [ '0.0.0.0:8888' ]

    processors:
      memory_limiter:
        check_interval: 1s
        limit_percentage: 80
        spike_limit_percentage: 25
      batch: {}
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        # filter:
        #   # only retrieve pods running on the same node as the collector
        #   node_from_env_var: KUBE_NODE_NAME
        extract:
          # The attributes provided in 'metadata' will be added to associated resources
          # metadata:
          #   - k8s.pod.name
          #   - k8s.pod.uid
          #   - k8s.deployment.name
          #   - k8s.namespace.name
          #   - k8s.node.name
          #   - k8s.pod.start_time
          #   - service.namespace
          #   - service.name
          #   - service.version
          #   - service.instance.id
          labels:
            # This label extraction rule takes the value 'app.kubernetes.io/component' label and maps it to the 'app.label.component' attribute which will be added to the associated resources
            - tag_name: test.label
              key: test.label
              from: pod
          # otel_annotations: true
        pod_association:
          - sources:
              # This rule associates all resources containing the 'k8s.pod.ip' attribute with the matching pods. If this attribute is not present in the resource, this rule will not be able to find the matching pod.
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              # This rule associates all resources containing the 'k8s.pod.uid' attribute with the matching pods. If this attribute is not present in the resource, this rule will not be able to find the matching pod.
              - from: resource_attribute
                name: k8s.pod.uid
          # - sources:
          #     # This rule will use the IP from the incoming connection from which the resource is received, and find the matching pod, based on the 'pod.status.podIP' of the observed pods
          #     - from: connection

      resource:
        attributes:
          - action: upsert
            key: newrelic.entity.type
            value: k8s

      {{- range $teamName, $teamInfo := $teams }}
      resource/{{ $teamName }}:
        attributes:
          - key: "collector.source"
            action: "insert"
            value: "{{ $teamName}}"
      {{- end }}

    connectors:
      routing/metrics:
        default_pipelines: [metrics/default]
        table:
          {{- range $teamName, $teamInfo := $teams }}
          - context: metric
            condition: 'resource.attributes["k8s.namespace.name"] == "{{ $teamName }}"{{ if $teamInfo.identifier }} or resource.attributes["{{ $teamInfo.identifier.key }}"] == "{{ $teamInfo.identifier.value }}"{{ end }}'
            pipelines:
              - metrics/{{ $teamName }}
          {{- end }}

    exporters:
      debug: {}
      {{- range $teamName, $teamInfo := $teams }}
      otlphttp/{{ $teamName }}:
        endpoint: {{ $teamInfo.endpoint}}
        headers:
          api-key: ${NR_LICENSE_KEY_{{ $teamName | upper | replace "-" "_" }}}
      {{- end }}

    service:
      pipelines:
        metrics/default:
          receivers: [routing/metrics]
          processors: [memory_limiter, resource, resource/platform, batch]
          # The routing connector acts as an exporter for the first pipeline...
          exporters: [debug]
        metrics/ingress:
          receivers: [prometheus]
          processors: [memory_limiter, batch]
          # The routing connector acts as an exporter for the first pipeline...
          exporters: [routing/metrics]
        {{- range $teamName, $teamInfo := $teams }}
        metrics/{{ $teamName}}:
          receivers: [routing/metrics]
          processors: [memory_limiter, resource, resource/{{ $teamName }}, k8sattributes, batch]
          exporters: [debug]
        {{- end }}