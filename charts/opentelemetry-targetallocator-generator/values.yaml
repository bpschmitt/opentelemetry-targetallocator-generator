# Default values for opentelemetry-targetallocator-generator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Number of replicas
replicas: 1

targetAllocatorName: ""

fullnameOverride: ""

global:
  # New Relic account configuration
  newrelic:
    # OTLP endpoint for all New Relic accounts
    # For US accounts -> https://otlp.nr-data.net
    # For EU accounts -> https://otlp.eu01.nr-data.net
    endpoint: "https://otlp.nr-data.net"
    # Teams to segragete the telemetry data received by all of the collectors.
    teams:
      team-a:
        licenseKey:
          secretRef:
            name: newrelic-license-key-app-team-a
            key: license
        identifier:
          key: test.label
          value: team-a
        namespaces:
          - team-a
      team-b:
        licenseKey:
          secretRef:
            name: newrelic-license-key-app-team-b
            key: license
        identifier:
          key: test.label
          value: team-b
        namespaces:
          - team-b
      # team-c:
      #   licenseKey:
      #     secretRef:
      #       name: newrelic-license-key-app-team-c
      #       key: license
      #   identifier:
      #     key: test.label
      #     value: team-c
      #   namespaces:
      #     - team-c
      # team-d:
      #   licenseKey:
      #     secretRef:
      #       name: newrelic-license-key-app-team-d
      #       key: license
      #   identifier:
      #     key: test.label
      #     value: team-d
      #   namespaces:
      #     - team-d

# Image
image:
  # Repository
  repository: otel/opentelemetry-collector-contrib
  # Image pull policy
  pullPolicy: IfNotPresent
  # Image tag
  tag: "0.137.0"

# Service account
serviceAccount:
  # Annotations to add to the service account
  annotations:  {}

# Security context for container priviliges
securityContext: {}

# Annotations for collector pods
annotations: {}

clusterRole:
  # Annotations to add to the clusterRole
  # Can be used in combination with presets that create a cluster role.
  annotations: {}
  # A set of rules as documented here : https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  # Can be used in combination with presets that create a cluster role to add additional rules.
  # Rules sourced from https://github.com/open-telemetry/opentelemetry-operator/blob/main/cmd/otel-allocator/README.md#rbac
  rules:
    - apiGroups: [""]
      resources:
      - nodes
      - nodes/metrics
      - services
      - endpoints
      - pods
      verbs: ["get", "list", "watch"]
    - apiGroups: [""]
      resources:
      - configmaps
      verbs: ["get"]
    - apiGroups:
      - discovery.k8s.io
      resources:
      - endpointslices
      verbs: ["get", "list", "watch"]
    - apiGroups:
      - networking.k8s.io
      resources:
      - ingresses
      verbs: ["get", "list", "watch"]
    - nonResourceURLs: ["/metrics"]
      verbs: ["get"]
    - apiGroups:
      - monitoring.coreos.com
      resources:
      - servicemonitors
      - podmonitors
      verbs:
      - '*'
    - apiGroups: [""]
      resources:
      - namespaces
      verbs: ["get", "list", "watch"]

clusterRoleBinding:
  # Annotations to add to the clusterRoleBinding
  # Can be used in combination with presets that create a cluster role binding.
  annotations: {}
# Array of key value pairs defining the ports for the
# collector to expose
ports:
  # Prometheus
  prometheus:
    name: prometheus
    protocol: TCP
    port: 8888
    targetPort: 8888

# Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
resources:
  requests:
    cpu: 100m
    memory: 256Mi
  limits:
    memory: 512Mi

# Target allocator
targetAllocator:
  # Strategy to filter the discovered targets before distributing them across collectors.
  # Options:
  # - relabel-config (default)
  filterStrategy: relabel-config
  # Strategy to distribute targets across collectors.
  # Options:
  # - consistent-hashing (default)
  # - least-weighted
  allocationStrategy: consistent-hashing

  prometheusCR:
    enabled: true
  podMonitorSelector: {}
  serviceMonitorSelector: {}
